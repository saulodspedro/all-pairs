{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/saulo/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/saulo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns=['I','you','he','she','it','we','they','me','him','her','us','them','what','who','whom','mine','yours','his','hers','ours','theirs','this','that','these','those','who','whom','which','what','whose','whoever','whatever','whichever','whomever','myself','yourself','himself','herself','itself','ourselves','themselves','each other','one another','anything','everybody','another','each','few','many','none','some','all','any','anybody','anyone','everyone','everything','no one','nobody','nothing','none','other','others','several','somebody','someone','something','most','enough','little','more','both','either','neither','one','much','such']\n",
    "\n",
    "ignored_words = ['part','and','a','the end','end','parts','use','s']\n",
    "\n",
    "right_noun_pattern = '^(NOUN|PROPN)*(VERB)+(ADJ|ADP|DT)+$|^(((NOUN)+(ADJ)+|(ADJ)+(NOUN)+)(ADJ|NOUN)*(ADJ|ADP|DET)+)$'\n",
    "left_noun_pattern = '^VERB\\w*NOUN$|^(VERB)+ADP$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load time: 0.00025200843811035156\n",
      "parser: 0.1034095287322998\n",
      "get nouns: 0.003258228302001953\n",
      "clean nouns: 0.0006132125854492188\n",
      "build grams: 0.021496295928955078\n",
      "build pairs: 0.0241391658782959\n",
      "0.15362143516540527\n"
     ]
    }
   ],
   "source": [
    "#simple all-pairs data generator\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#filename = '/home/saulo/projects/all-pairs/data/AA/wiki_00'\n",
    "filename = '/home/saulo/projects/all-pairs/samples/queen.txt'\n",
    "\n",
    "cp1 = time.time()\n",
    "\n",
    "with open(filename, 'r') as myfile:\n",
    "    data=myfile.read().replace('\\n', ' ')\n",
    "    \n",
    "cp2 = time.time()\n",
    "print('load time: '+str(cp2-cp1))\n",
    "        \n",
    "spacy_obj = nlp(data)\n",
    "textblob_obj = TextBlob(data)\n",
    "\n",
    "cp3 = time.time()\n",
    "print('parser: '+str(cp3-cp2))\n",
    "\n",
    "del data\n",
    "\n",
    "nouns = [chunk.text.lower() for chunk in spacy_obj.noun_chunks]\n",
    "\n",
    "cp4 = time.time()\n",
    "print('get nouns: '+str(cp4-cp3))\n",
    "\n",
    "clean_nouns = [n for n in nouns if n not in pronouns]  #remove pronouns\n",
    "\n",
    "cp5 = time.time()\n",
    "print('clean nouns: '+str(cp5-cp4))\n",
    "\n",
    "del spacy_obj\n",
    "\n",
    "everygrams = nltk.everygrams(textblob_obj.words,\n",
    "                             min_len=3,\n",
    "                             max_len=5)\n",
    "\n",
    "cp6 = time.time()\n",
    "print('build grams: '+str(cp6-cp5))\n",
    "\n",
    "del textblob_obj\n",
    "\n",
    "pairs = []\n",
    "\n",
    "set_nouns = set(clean_nouns)\n",
    "\n",
    "for gram in everygrams:\n",
    "#    set_grams = set([g.lower() for g in grams])\n",
    "     \n",
    "    \n",
    "    explode_gram = nltk.everygrams(gram,\n",
    "                                  min_len=1,\n",
    "                                  max_len=len(gram)-2)  #avoid context patterns with a single word\n",
    "    \n",
    "    ctx_candidates = [' '.join(g).lower() for g in explode_gram]\n",
    "    \n",
    "    intersection = list(set(ctx_candidates).intersection(set_nouns))\n",
    "    \n",
    "    joined = ' '.join(gram).lower()\n",
    "    for noun in intersection:\n",
    "        if joined.startswith(noun+\" \") or joined.endswith(\" \"+noun):\n",
    "            pairs.append((noun, joined.replace(noun,'_',1)))\n",
    "            \n",
    "cp7 = time.time()\n",
    "print('build pairs: '+str(cp7-cp6))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load time: 0.0005166530609130859\n",
    "parser: 0.11905574798583984\n",
    "get nouns: 0.0037605762481689453\n",
    "clean nouns: 0.0006110668182373047\n",
    "build grams: 0.021936416625976562\n",
    "build pairs: 0.2348949909210205\n",
    "0.3810858726501465\n",
    "\n",
    "#    for word in grams[:1] + grams[-1:]:\n",
    "#        if word in inter:\n",
    "#            pairs.append((word, ' '.join(grams).replace(word,'_',1)))\n",
    "\n",
    "#for grams in everygrams:\n",
    "#    joined = ' '.join(grams).lower()\n",
    "#    for noun in clean_nouns:\n",
    "#        if ((len(joined.split()) >= len(noun.split()) + 2) and  #avoid context patterns with a single word \n",
    "#            (joined.startswith(noun+\" \") or \n",
    "#             joined.endswith(\" \"+noun))):\n",
    "#            pairs.append((noun, joined.replace(noun,'_',1)))\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = ['one','two','three','four','five','six','seven']\n",
    "\n",
    "['one','two','three']\n",
    "\n",
    "['two','three','four']\n",
    "\n",
    "['four', 'five', 'six']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairs(filename, nlp_remote):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #load data\n",
    "    with open(filename, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', ' ')\n",
    "    \n",
    "    #get noun phrases\n",
    "    spacy_obj = nlp_remote(data)\n",
    "    \n",
    "    #remove puctuation\n",
    "    tokens = [t for t in spacy_obj.doc if t.pos_ != 'PUNCT']\n",
    "    \n",
    "    nouns = [chunk.text.lower() for chunk in spacy_obj.noun_chunks]\n",
    "    set_nouns = set([n for n in nouns if n not in pronouns + ignored_words])  #remove pronouns\n",
    "    \n",
    "    del nouns\n",
    "    del spacy_obj\n",
    "\n",
    "    #get candidate contexts\n",
    "    everygrams = nltk.everygrams(tokens,\n",
    "                                 min_len=3,\n",
    "                                 max_len=5)\n",
    "    \n",
    "    #build pairs\n",
    "    pairs = []\n",
    "    \n",
    "    for gram in everygrams:\n",
    "        first_word = gram[0]\n",
    "        last_word = gram[-1]\n",
    "        \n",
    "        explode_gram = list(nltk.everygrams(gram, min_len=1))\n",
    "        \n",
    "        for eg in explode_gram:\n",
    "            if(len(eg) <= len(gram)-2):\n",
    "                \n",
    "                eg_string = ' '.join([g.text for g in eg]).lower()\n",
    "                \n",
    "                if(eg_string in set_nouns):\n",
    "                    if(eg[0] == first_word):\n",
    "                        context = [gr for gr in explode_gram if len(gr) == len(gram)-len(eg) and gr[-1] == last_word]\n",
    "                        ctx_pos_pattern = ''.join([c.pos_ for c in context[0]])\n",
    "                            \n",
    "                        if(re.search(left_noun_pattern, ctx_pos_pattern)):\n",
    "                            ctx_string = ' '.join([c.text for c in context[0]])\n",
    "                            pairs.append((eg_string, \"_ \"+ctx_string))\n",
    "                \n",
    "                    if(eg[-1] == last_word):\n",
    "                        context = [gr for gr in explode_gram if len(gr) == len(gram)-len(eg) and gr[0] == first_word]\n",
    "                        ctx_pos_pattern = ''.join([c.pos_ for c in context[0]])\n",
    "                        \n",
    "                        if(re.search(right_noun_pattern, ctx_pos_pattern)):\n",
    "                            ctx_string = ' '.join([c.text for c in context[0]])\n",
    "                            pairs.append((eg_string, ctx_string+\" _\"))\n",
    "    \n",
    "    del set_nouns\n",
    "    \n",
    "    #save to database\n",
    "    res = insert_pairs(pairs)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    #print results\n",
    "    print('{}: {} {} {}s'.format(datetime.datetime.now(),\n",
    "                                 filename,\n",
    "                                 len(res.inserted_ids),\n",
    "                                 end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy settings\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'queen ii', 'rock history', 'freddie mercury', 'number', 'the rock and roll hall', \"the world's best-selling music artists\", 'nine weeks', 'queen', 'the bandâ€™s 1977 album news', 'more conventional and radio-friendly works', 'international success', 'their classic line-up', 'further styles', 'various publications', 'the 1985 live aid concert', 'the uk', 'vocalists', 'estimates', '\"queen', 'brian may', 'the british phonographic industry', 'a fan', 'mercury', 'the champions', 'arena rock', 'vocals', '\"bohemian rhapsody', 'the band smile', 'their 1981 compilation album', 'piano', 'knebworth', 'greatest hits', 'the biggest stadium rock bands', 'the opera', 'the dust', 'his last performance', 'the outstanding contribution', 'a complication', 'their second album', 'the band', 'the music video', 'the us', 'bass guitar', 'august', 'the songwriters hall', 'the queen name', 'may', 'sheer heart attack', 'events', 'smile', 'drums', 'taylor', 'heavy metal', 'more elaborate stage', 'pop rock', 'the ivor novello award', 'progressive rock', '170 million to 300 million records', 'aids', 'british music award', 'recording techniques', 'anthems', 'their performance', 'the name', 'composers', 'roger taylor', 'england', 'songwriters', 'hard rock', 'deacon', 'the british academy', '\"we', 'fame', 'authors', 'lead vocals', 'the best-selling album', 'their eponymous debut album', 'outstanding song collection', 'hit singles', 'london', 'their earliest works', 'bronchopneumonia', 'the world', 'paul rodgers', 'the grammy lifetime achievement award', 'each member', 'a british rock band', 'adam lambert', 'john deacon', 'lead guitar'}\n",
      "[('london', 'formed in _'), ('may', '_ lead guitar'), ('number', 'stayed at _'), ('bronchopneumonia', 'died of _'), ('deacon', '_ retired in'), ('london', 'band formed in _'), ('brian may', '_ lead guitar'), ('progressive rock', 'influenced by _'), ('mercury', '_ was a fan'), ('deacon', '_ was recruited before'), ('the band', 'recruited before _'), ('the uk', 'charted in _'), ('knebworth', 'last performance at _'), ('taylor', '_ have performed under'), ('queen', '_ are a British rock'), ('london', 'rock band formed in _'), ('may', '_ lead guitar and vocals'), ('progressive rock', 'were influenced by _'), ('arena rock', 'styles such as _'), ('more elaborate stage', 'experiment with _'), ('the band', 'was recruited before _'), ('mercury', '_ gave his last performance'), ('the queen name', 'performed under _'), ('each member', '_ has composed hit'), ('the songwriters hall', 'inducted into _')]\n",
      "0.04464578628540039\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/saulo/projects/all-pairs/samples/queen.txt'\n",
    "build_pairs(filename, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'e']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = ['b','c','d']\n",
    "mylist = ['a','b','c','d','e','b']\n",
    "x = list(filter(lambda x: x not in pattern, mylist))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gram in everygrams:\n",
    "#    set_grams = set([g.lower() for g in grams])\n",
    "     \n",
    "    \n",
    "    explode_gram = nltk.everygrams(gram,\n",
    "                                  min_len=1,\n",
    "                                  max_len=len(gram)-2)  #avoid context patterns with a single word\n",
    "    \n",
    "    ctx_candidates = [' '.join(g).lower() for g in explode_gram]\n",
    "    \n",
    "    intersection = list(set(ctx_candidates).intersection(set_nouns))\n",
    "    \n",
    "    joined = ' '.join(gram).lower()\n",
    "    for noun in intersection:\n",
    "        if joined.startswith(noun+\" \") or joined.endswith(\" \"+noun):\n",
    "            pairs.append((noun, joined.replace(noun,'_',1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "y = [1,3,4,5,6]\n",
    "\n",
    "for yi in y:\n",
    "    print(y.index(yi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "x = nlp(\"Alice broke the home run record\")\n",
    "\n",
    "for tk in x:\n",
    "    print(tk.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7fa0bd599840>\n"
     ]
    }
   ],
   "source": [
    "print(xi.pos_ for xi in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

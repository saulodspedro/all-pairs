{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/saulo/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/saulo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns=['I','you','he','she','it','we','they','me','him','her','us','them','what','who','whom','mine','yours','his','hers','ours','theirs','this','that','these','those','who','whom','which','what','whose','whoever','whatever','whichever','whomever','myself','yourself','himself','herself','itself','ourselves','themselves','each other','one another','anything','everybody','another','each','few','many','none','some','all','any','anybody','anyone','everyone','everything','no one','nobody','nothing','none','other','others','several','somebody','someone','something','most','enough','little','more','both','either','neither','one','much','such']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load time: 0.00025200843811035156\n",
      "parser: 0.1034095287322998\n",
      "get nouns: 0.003258228302001953\n",
      "clean nouns: 0.0006132125854492188\n",
      "build grams: 0.021496295928955078\n",
      "build pairs: 0.0241391658782959\n",
      "0.15362143516540527\n"
     ]
    }
   ],
   "source": [
    "#simple all-pairs data generator\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#filename = '/home/saulo/projects/all-pairs/data/AA/wiki_00'\n",
    "filename = '/home/saulo/projects/all-pairs/samples/queen.txt'\n",
    "\n",
    "cp1 = time.time()\n",
    "\n",
    "with open(filename, 'r') as myfile:\n",
    "    data=myfile.read().replace('\\n', ' ')\n",
    "    \n",
    "cp2 = time.time()\n",
    "print('load time: '+str(cp2-cp1))\n",
    "        \n",
    "spacy_obj = nlp(data)\n",
    "textblob_obj = TextBlob(data)\n",
    "\n",
    "cp3 = time.time()\n",
    "print('parser: '+str(cp3-cp2))\n",
    "\n",
    "del data\n",
    "\n",
    "nouns = [chunk.text.lower() for chunk in spacy_obj.noun_chunks]\n",
    "\n",
    "cp4 = time.time()\n",
    "print('get nouns: '+str(cp4-cp3))\n",
    "\n",
    "clean_nouns = [n for n in nouns if n not in pronouns]  #remove pronouns\n",
    "\n",
    "cp5 = time.time()\n",
    "print('clean nouns: '+str(cp5-cp4))\n",
    "\n",
    "del spacy_obj\n",
    "\n",
    "everygrams = nltk.everygrams(textblob_obj.words,\n",
    "                             min_len=3,\n",
    "                             max_len=5)\n",
    "\n",
    "cp6 = time.time()\n",
    "print('build grams: '+str(cp6-cp5))\n",
    "\n",
    "del textblob_obj\n",
    "\n",
    "pairs = []\n",
    "\n",
    "set_nouns = set(clean_nouns)\n",
    "\n",
    "for gram in everygrams:\n",
    "#    set_grams = set([g.lower() for g in grams])\n",
    "     \n",
    "    \n",
    "    explode_gram = nltk.everygrams(gram,\n",
    "                                  min_len=1,\n",
    "                                  max_len=len(gram)-2)  #avoid context patterns with a single word\n",
    "    \n",
    "    ctx_candidates = [' '.join(g).lower() for g in explode_gram]\n",
    "    \n",
    "    intersection = list(set(ctx_candidates).intersection(set_nouns))\n",
    "    \n",
    "    joined = ' '.join(gram).lower()\n",
    "    for noun in intersection:\n",
    "        if joined.startswith(noun+\" \") or joined.endswith(\" \"+noun):\n",
    "            pairs.append((noun, joined.replace(noun,'_',1)))\n",
    "            \n",
    "cp7 = time.time()\n",
    "print('build pairs: '+str(cp7-cp6))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load time: 0.0005166530609130859\n",
    "parser: 0.11905574798583984\n",
    "get nouns: 0.0037605762481689453\n",
    "clean nouns: 0.0006110668182373047\n",
    "build grams: 0.021936416625976562\n",
    "build pairs: 0.2348949909210205\n",
    "0.3810858726501465\n",
    "\n",
    "#    for word in grams[:1] + grams[-1:]:\n",
    "#        if word in inter:\n",
    "#            pairs.append((word, ' '.join(grams).replace(word,'_',1)))\n",
    "\n",
    "#for grams in everygrams:\n",
    "#    joined = ' '.join(grams).lower()\n",
    "#    for noun in clean_nouns:\n",
    "#        if ((len(joined.split()) >= len(noun.split()) + 2) and  #avoid context patterns with a single word \n",
    "#            (joined.startswith(noun+\" \") or \n",
    "#             joined.endswith(\" \"+noun))):\n",
    "#            pairs.append((noun, joined.replace(noun,'_',1)))\n",
    "#            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
